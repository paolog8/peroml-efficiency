{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efecae6-f8b3-4d4a-8f6b-a3674d2ba73e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir(\"/home/jovyan/work/\")\n",
    "os.getcwd()\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e049cf7-c827-4ec1-96d3-ceb27d7a395e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ydata_profiling\n",
    "import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def profile_dataset(df: pd.DataFrame, dataset_name: str, target_cols: list = None, config_file: str = 'config.txt', output_dir: str = 'reports') -> pd.DataFrame:\n",
    "    # Save the report as an HTML file with the dataset name as the file name\n",
    "    profile.to_file(output_file=output_path)\n",
    "    \n",
    "    # Append the dataset name and target columns to the configuration file\n",
    "    with open(os.path.join(output_dir, config_file), 'a') as f:\n",
    "        f.write(f'{dataset_name}: {\",\".join(sorted(target_cols))}\\n')\n",
    "    \"\"\"\n",
    "    Generate a Pandas profiling report for a DataFrame, allowing for one or more columns to be selected as target variables.\n",
    "    Append dataset name and columns used to write the report in a configuration file, and include a timestamp in the report name.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    dataset_name (str): Name of the dataset.\n",
    "    target_cols (list of str): A list of column names to use as target variables. If None, all columns are included.\n",
    "    config_file (str): Name of the configuration file to append.\n",
    "    output_dir (str): Directory to save the report file.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler(os.path.join(output_dir,'profile_dataset.log'))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Check that the input parameters are valid\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError('Input parameter `df` must be a pandas DataFrame.')\n",
    "    if not isinstance(dataset_name, str):\n",
    "        raise ValueError('Input parameter `dataset_name` must be a string.')\n",
    "    if target_cols is not None and not isinstance(target_cols, list):\n",
    "        raise ValueError('Input parameter `target_cols` must be a list of strings.')\n",
    "    if not isinstance(config_file, str):\n",
    "        raise ValueError('Input parameter `config_file` must be a string.')\n",
    "    if not isinstance(output_dir, str):\n",
    "        raise ValueError('Input parameter `output_dir` must be a string.')\n",
    "    \n",
    "    # If target_cols is None, use all columns\n",
    "    if target_cols is None:\n",
    "        target_cols = df.columns\n",
    "    \n",
    "    # Select only the target columns\n",
    "    df_target = df[target_cols]\n",
    "\n",
    "    # If the config file does not exist, create it\n",
    "    if not os.path.isfile(os.path.join(output_dir, config_file)):\n",
    "        with open(os.path.join(output_dir, config_file), 'w') as f:\n",
    "            f.write('# Dataset name and target columns used for each report\\n')\n",
    "    \n",
    "    # Check if the same set of target columns has already been used for another dataset, or if the report file already exists\n",
    "    skip_report_generation = False\n",
    "\n",
    "    with open(os.path.join(output_dir, config_file), 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line.startswith('#') and ':' in line:\n",
    "                other_dataset, other_cols = line.split(':')\n",
    "                other_cols = set(other_cols.strip().split(','))\n",
    "                if dataset_name == other_dataset and set(target_cols) == other_cols:\n",
    "                    logger.warning(f'Same dataset name and target columns already used. Skipping report generation.')\n",
    "                    skip_report_generation = True\n",
    "                    break\n",
    "\n",
    "    if skip_report_generation:\n",
    "        return df_target\n",
    "\n",
    "    \n",
    "\n",
    "    # Generate a Pandas profiling report for the target columns\n",
    "    profile = df_target.profile_report(title=f'{dataset_name} Analysis Report')\n",
    "    \n",
    "    # Save the report as an HTML file with the dataset name as the file name\n",
    "    report_name = f'{dataset_name}_report.html'\n",
    "    output_path = os.path.join(output_dir, report_name)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Log information about the function's execution\n",
    "    logger.info(f'Generated profiling report for dataset {dataset_name} with target columns {target_cols}.')\n",
    "    return df_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a20d5c3-edf4-4a49-a3de-574f1ff8c662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from dython.nominal import theils_u\n",
    "\n",
    "def compute_correlations(df: pd.DataFrame, target_col: str, missing_values: str = 'drop') -> dict:\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation coefficient, Theil's U and mutual information between a target column and all other columns in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    target_col (str): Name of the target column\n",
    "    missing_values (str): How to handle missing values. 'drop' to remove missing values or 'fill' to replace missing values with column mean.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of column names and correlation measures\n",
    "    \"\"\"\n",
    "    # Drop columns with more than 90% missing values\n",
    "    df.dropna(thresh=len(df)*0.1, axis=1, inplace=True)\n",
    "    \n",
    "    # Drop constant columns\n",
    "    df = df.loc[:, (df != df.iloc[0]).any()]\n",
    "    \n",
    "    # Select only the numeric columns\n",
    "    df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "    \n",
    "    if missing_values == 'drop':\n",
    "        # Drop rows with missing values\n",
    "        df_numeric = df_numeric.dropna()\n",
    "    elif missing_values == 'fill':\n",
    "        # Fill missing values with column mean\n",
    "        df_numeric = df_numeric.fillna(df_numeric.mean())\n",
    "    \n",
    "    # Compute Pearson correlation coefficient between target column and all other numeric columns\n",
    "    pearson_corr = {}\n",
    "    for col in df_numeric.columns:\n",
    "        if col != target_col:\n",
    "            corr, _ = pearsonr(df_numeric[target_col], df_numeric[col])\n",
    "            pearson_corr[col] = corr\n",
    "    \n",
    "    # Compute mutual information between target column and all other numeric columns\n",
    "    mi = mutual_info_regression(df_numeric.drop(columns=[target_col]), df_numeric[target_col])\n",
    "    mi_dict = dict(zip(df_numeric.drop(columns=[target_col]).columns, mi))\n",
    "    \n",
    "    # Compute Theil's U between target column and all other non-numeric columns\n",
    "    theils_u_dict = {}\n",
    "    for col in df.columns:\n",
    "        if col != target_col and col not in df_numeric.columns:\n",
    "            theils_u_val = theils_u(df[target_col], df[col])\n",
    "            theils_u_dict[col] = theils_u_val\n",
    "    \n",
    "    # Combine the dictionaries of correlation measures\n",
    "    corr_dict = {}\n",
    "    for col in df.columns:\n",
    "        if col in pearson_corr.keys():\n",
    "            corr_dict[col] = {'pearson_corr': pearson_corr[col], 'mutual_info': mi_dict[col]}\n",
    "        elif col in theils_u_dict.keys():\n",
    "            corr_dict[col] = {'theils_u': theils_u_dict[col]}\n",
    "    \n",
    "    return corr_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f7efb2b-3254-4c0e-8569-2e503750f35a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def csv_to_dict(filepath):\n",
    "    \"\"\"\n",
    "    Reads a csv file with \";\" as separators and uses the first row as the index.\n",
    "    For each row, outputs a dictionary where each entry is one of the possible values in the row,\n",
    "    and the value is a list of columns where we find that value.\n",
    "\n",
    "    Args:\n",
    "    - filepath (str): the path to the csv file\n",
    "\n",
    "    Returns:\n",
    "    - data_list (list): a list of dictionaries where each dictionary represents a row in the csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    import csv\n",
    "    \n",
    "    data_list = []\n",
    "\n",
    "    with open(filepath, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        headers = next(reader) # Store headers as list and move to next line\n",
    "\n",
    "        for row in reader:\n",
    "            row_dict = {}\n",
    "            for index, value in enumerate(row):\n",
    "                if value not in row_dict:\n",
    "                    row_dict[value] = [headers[index]]\n",
    "                else:\n",
    "                    row_dict[value].append(headers[index])\n",
    "            data_list.append(row_dict)\n",
    "\n",
    "    return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7fcadbd-cdac-4ec3-9b7b-bc1bf4d2114b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ref_publication_date',\n",
       " 'Cell_area_measured',\n",
       " 'Cell_architecture',\n",
       " 'Cell_flexible',\n",
       " 'Module',\n",
       " 'Module_area_total',\n",
       " 'Substrate_stack_sequence',\n",
       " 'ETL_stack_sequence',\n",
       " 'ETL_thickness',\n",
       " 'ETL_additives_compounds',\n",
       " 'ETL_deposition_procedure',\n",
       " 'ETL_surface_treatment_before_next_deposition_step',\n",
       " 'Perovskite_dimension_2D',\n",
       " 'Perovskite_dimension_3D',\n",
       " 'Perovskite_dimension_3D_with_2D_capping_layer',\n",
       " 'Perovskite_composition_perovskite_ABC3_structure',\n",
       " 'Perovskite_composition_a_ions',\n",
       " 'Perovskite_composition_a_ions_coefficients',\n",
       " 'Perovskite_composition_b_ions',\n",
       " 'Perovskite_composition_b_ions_coefficients',\n",
       " 'Perovskite_composition_c_ions',\n",
       " 'Perovskite_composition_c_ions_coefficients',\n",
       " 'Perovskite_composition_none_stoichiometry_components_in_excess',\n",
       " 'Perovskite_additives_compounds',\n",
       " 'Perovskite_additives_concentrations',\n",
       " 'Perovskite_thickness',\n",
       " 'Perovskite_band_gap',\n",
       " 'Perovskite_band_gap_graded',\n",
       " 'Perovskite_pl_max',\n",
       " 'Perovskite_deposition_number_of_deposition_steps',\n",
       " 'Perovskite_deposition_procedure',\n",
       " 'Perovskite_deposition_synthesis_atmosphere',\n",
       " 'Perovskite_deposition_solvents',\n",
       " 'Perovskite_deposition_solvents_mixing_ratios',\n",
       " 'Perovskite_deposition_quenching_induced_crystallisation',\n",
       " 'Perovskite_deposition_quenching_media',\n",
       " 'Perovskite_deposition_quenching_media_mixing_ratios',\n",
       " 'Perovskite_deposition_quenching_media_additives_compounds',\n",
       " 'Perovskite_deposition_thermal_annealing_temperature',\n",
       " 'Perovskite_deposition_thermal_annealing_time',\n",
       " 'Perovskite_deposition_thermal_annealing_atmosphere',\n",
       " 'Perovskite_deposition_solvent_annealing',\n",
       " 'Perovskite_deposition_solvent_annealing_solvent_atmosphere',\n",
       " 'Perovskite_deposition_after_treatment_of_formed_perovskite',\n",
       " 'Perovskite_surface_treatment_before_next_deposition_step',\n",
       " 'HTL_stack_sequence',\n",
       " 'HTL_thickness_list',\n",
       " 'HTL_additives_compounds',\n",
       " 'HTL_additives_concentrations',\n",
       " 'HTL_deposition_procedure',\n",
       " 'HTL_deposition_synthesis_atmosphere',\n",
       " 'HTL_deposition_solvents',\n",
       " 'HTL_deposition_solvents_mixing_ratios',\n",
       " 'Backcontact_stack_sequence',\n",
       " 'Backcontact_thickness_list',\n",
       " 'Backcontact_deposition_procedure',\n",
       " 'Add_lay_front_stack_sequence',\n",
       " 'Add_lay_back',\n",
       " 'Encapsulation',\n",
       " 'Encapsulation_stack_sequence',\n",
       " 'JV_light_intensity',\n",
       " 'JV_light_spectra',\n",
       " 'JV_scan_speed',\n",
       " 'JV_scan_delay_time',\n",
       " 'JV_scan_integration_time',\n",
       " 'JV_preconditioning_protocol',\n",
       " 'JV_preconditioning_time',\n",
       " 'JV_preconditioning_potential',\n",
       " 'JV_default_PCE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = csv_to_dict(\"data/external/relevant_features.csv\")\n",
    "data_list[1][\"1\"] +  data_list[1]['Target (first)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2775cfd4-29e9-4792-a46f-7b6c82167f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28713/286321140.py:1: DtypeWarning: Columns (10,22,29,31,32,35,36,40,44,45,46,48,51,54,65,84,89,90,93,98,99,100,105,108,115,118,122,123,125,130,134,138,142,143,144,146,149,152,163,166,167,171,172,173,175,178,181,192,194,225,271,272,273,277,304,315,321,325,330,331,335,336,342,348,369,371,373,374,376,380,384,387,403,405,407,409) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(\"data/raw/Perovskite_database_all_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/raw/Perovskite_database_all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dbf6a0-8fe4-4407-a015-16db5fcae956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ref_ID</th>\n",
       "      <th>Ref_ID_temp</th>\n",
       "      <th>Ref_name_of_person_entering_the_data</th>\n",
       "      <th>Ref_data_entered_by_author</th>\n",
       "      <th>Ref_DOI_number</th>\n",
       "      <th>Ref_lead_author</th>\n",
       "      <th>Ref_publication_date</th>\n",
       "      <th>Ref_journal</th>\n",
       "      <th>Ref_part_of_initial_dataset</th>\n",
       "      <th>Ref_original_filename_data_upload</th>\n",
       "      <th>...</th>\n",
       "      <th>Outdoor_PCE_Tse80</th>\n",
       "      <th>Outdoor_PCE_after_1000_h</th>\n",
       "      <th>Outdoor_power_generated</th>\n",
       "      <th>Outdoor_link_raw_data_for_outdoor_trace</th>\n",
       "      <th>Outdoor_detaild_weather_data_available</th>\n",
       "      <th>Outdoor_link_detailed_weather_data</th>\n",
       "      <th>Outdoor_spectral_data_available</th>\n",
       "      <th>Outdoor_link_spectral_data</th>\n",
       "      <th>Outdoor_irradiance_measured</th>\n",
       "      <th>Outdoor_link_irradiance_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam Hultqvist</td>\n",
       "      <td>False</td>\n",
       "      <td>10.1021/jp5126624</td>\n",
       "      <td>Sabba et al.</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>The Journal of Physical Chemistry C</td>\n",
       "      <td>True</td>\n",
       "      <td>Historic dataset on 2020 11 22_v7.xlsx</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam Hultqvist</td>\n",
       "      <td>False</td>\n",
       "      <td>10.1021/jp5126624</td>\n",
       "      <td>Sabba et al.</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>The Journal of Physical Chemistry C</td>\n",
       "      <td>True</td>\n",
       "      <td>Historic dataset on 2020 11 22_v7.xlsx</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam Hultqvist</td>\n",
       "      <td>False</td>\n",
       "      <td>10.1021/jp5126624</td>\n",
       "      <td>Sabba et al.</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>The Journal of Physical Chemistry C</td>\n",
       "      <td>True</td>\n",
       "      <td>Historic dataset on 2020 11 22_v7.xlsx</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Adam Hultqvist</td>\n",
       "      <td>False</td>\n",
       "      <td>10.1021/jp5126624</td>\n",
       "      <td>Sabba et al.</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>The Journal of Physical Chemistry C</td>\n",
       "      <td>True</td>\n",
       "      <td>Historic dataset on 2020 11 22_v7.xlsx</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Adam Hultqvist</td>\n",
       "      <td>False</td>\n",
       "      <td>10.1021/jp5126624</td>\n",
       "      <td>Sabba et al.</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>The Journal of Physical Chemistry C</td>\n",
       "      <td>True</td>\n",
       "      <td>Historic dataset on 2020 11 22_v7.xlsx</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ref_ID  Ref_ID_temp Ref_name_of_person_entering_the_data  \\\n",
       "0       1            1                       Adam Hultqvist   \n",
       "1       2            2                       Adam Hultqvist   \n",
       "2       3            3                       Adam Hultqvist   \n",
       "3       4            4                       Adam Hultqvist   \n",
       "4       5            5                       Adam Hultqvist   \n",
       "\n",
       "   Ref_data_entered_by_author     Ref_DOI_number Ref_lead_author  \\\n",
       "0                       False  10.1021/jp5126624    Sabba et al.   \n",
       "1                       False  10.1021/jp5126624    Sabba et al.   \n",
       "2                       False  10.1021/jp5126624    Sabba et al.   \n",
       "3                       False  10.1021/jp5126624    Sabba et al.   \n",
       "4                       False  10.1021/jp5126624    Sabba et al.   \n",
       "\n",
       "  Ref_publication_date                          Ref_journal  \\\n",
       "0           2015-01-06  The Journal of Physical Chemistry C   \n",
       "1           2015-01-06  The Journal of Physical Chemistry C   \n",
       "2           2015-01-06  The Journal of Physical Chemistry C   \n",
       "3           2015-01-06  The Journal of Physical Chemistry C   \n",
       "4           2015-01-06  The Journal of Physical Chemistry C   \n",
       "\n",
       "   Ref_part_of_initial_dataset       Ref_original_filename_data_upload  ...  \\\n",
       "0                         True  Historic dataset on 2020 11 22_v7.xlsx  ...   \n",
       "1                         True  Historic dataset on 2020 11 22_v7.xlsx  ...   \n",
       "2                         True  Historic dataset on 2020 11 22_v7.xlsx  ...   \n",
       "3                         True  Historic dataset on 2020 11 22_v7.xlsx  ...   \n",
       "4                         True  Historic dataset on 2020 11 22_v7.xlsx  ...   \n",
       "\n",
       "  Outdoor_PCE_Tse80 Outdoor_PCE_after_1000_h Outdoor_power_generated  \\\n",
       "0               NaN                      NaN                     NaN   \n",
       "1               NaN                      NaN                     NaN   \n",
       "2               NaN                      NaN                     NaN   \n",
       "3               NaN                      NaN                     NaN   \n",
       "4               NaN                      NaN                     NaN   \n",
       "\n",
       "   Outdoor_link_raw_data_for_outdoor_trace  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "\n",
       "   Outdoor_detaild_weather_data_available  Outdoor_link_detailed_weather_data  \\\n",
       "0                                   False                                 NaN   \n",
       "1                                   False                                 NaN   \n",
       "2                                   False                                 NaN   \n",
       "3                                   False                                 NaN   \n",
       "4                                   False                                 NaN   \n",
       "\n",
       "  Outdoor_spectral_data_available  Outdoor_link_spectral_data  \\\n",
       "0                           False                         NaN   \n",
       "1                           False                         NaN   \n",
       "2                           False                         NaN   \n",
       "3                           False                         NaN   \n",
       "4                           False                         NaN   \n",
       "\n",
       "   Outdoor_irradiance_measured  Outdoor_link_irradiance_data  \n",
       "0                        False                           NaN  \n",
       "1                        False                           NaN  \n",
       "2                        False                           NaN  \n",
       "3                        False                           NaN  \n",
       "4                        False                           NaN  \n",
       "\n",
       "[5 rows x 410 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329e6090-af06-43ff-aa00-189f541de2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'profile' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mprofile_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFirst_Iteration-Relevant_Columns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTarget (first)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36mprofile_dataset\u001b[0;34m(df, dataset_name, target_cols, config_file, output_dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprofile_dataset\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame, dataset_name: \u001b[38;5;28mstr\u001b[39m, target_cols: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, config_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, output_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreports\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Save the report as an HTML file with the dataset name as the file name\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mprofile\u001b[49m\u001b[38;5;241m.\u001b[39mto_file(output_file\u001b[38;5;241m=\u001b[39moutput_path)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Append the dataset name and target columns to the configuration file\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, config_file), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'profile' referenced before assignment"
     ]
    }
   ],
   "source": [
    "df=profile_dataset(df=dataset, dataset_name=\"First_Iteration-Relevant_Columns\",config_file=\"config.txt\", target_cols=data_list[1][\"1\"] +  data_list[1]['Target (first)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b12a8a-a575-495d-a3c7-11069a933e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9aa93b-2de3-4d6e-a36f-6b46842c7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def inspect_data(df):\n",
    "    \"\"\"\n",
    "    Inspects the input DataFrame for constant features, missing values, and outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    out_df (pandas.DataFrame): A DataFrame with the same shape as the input DataFrame, where True indicates that the\n",
    "    corresponding feature value is an outlier, and False indicates that it is not an outlier.\n",
    "    \"\"\"\n",
    "    # Check for constant features\n",
    "    constant_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    if len(constant_cols) > 0:\n",
    "        print(f'Found {len(constant_cols)} constant features: {\", \".join(constant_cols)}')\n",
    "    else:\n",
    "        print('No constant features found.')\n",
    "    \n",
    "    # Check for missing values\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(f'Found {df.isnull().sum().sum()} missing values.')\n",
    "    else:\n",
    "        print('No missing values found.')\n",
    "    \n",
    "    # Detect outliers using the interquartile range (IQR) method\n",
    "    # For each column, values outside the range [Q1 - 1.5*IQR, Q3 + 1.5*IQR] are considered outliers\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    is_outlier = ((df < lower_bound) | (df > upper_bound))\n",
    "    num_outliers = is_outlier.sum().sum()\n",
    "    if num_outliers > 0:\n",
    "        print(f'Found {num_outliers} outliers.')\n",
    "    else:\n",
    "        print('No outliers found.')\n",
    "    \n",
    "    # Create boolean mask for outliers\n",
    "    out_df = is_outlier.copy()\n",
    "    out_df.replace({True: 1, False: 0}, inplace=True)\n",
    "    \n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14653449-95dd-4e3d-83ab-19f97cc77315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def _infer_datatypes(df: pd.DataFrame) -> (list, list):\n",
    "    \"\"\"\n",
    "    Helper function to infer numeric and categorical columns from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    tuple: Lists of numeric and categorical column names.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "def group_rare_categories(series: pd.Series, threshold: float = 0.01) -> pd.Series:\n",
    "    \"\"\"Group rare categories in a pandas Series into an 'other' category.\"\"\"\n",
    "    category_counts = series.value_counts(normalize=True)\n",
    "    rare_categories = category_counts[category_counts < threshold].index\n",
    "    return series.apply(lambda x: 'other' if x in rare_categories else x)\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame, target_feature: str,\n",
    "                    num_imputer_strategy: str = 'median', \n",
    "                    cat_imputer_strategy: str = 'most_frequent', \n",
    "                    rare_category_threshold: float = 0.01) -> (pd.DataFrame, pd.Series):\n",
    "    \"\"\"\n",
    "    Preprocess the input dataset by handling missing values, outliers, and transforming categorical variables.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame to preprocess\n",
    "    target_feature (str): Name of the target feature\n",
    "    num_imputer_strategy (str): Imputer strategy for numeric columns (default: 'median')\n",
    "    cat_imputer_strategy (str): Imputer strategy for categorical columns (default: 'most_frequent')\n",
    "    rare_category_threshold (float): Threshold to group rare categories (default: 0.01)\n",
    "\n",
    "    Returns:\n",
    "    tuple: Preprocessed dataset (excluding the target feature) and the target Series.\n",
    "    \"\"\"\n",
    "    # Separate input features and target\n",
    "    X = df.drop(target_feature, axis=1)\n",
    "    y = df[target_feature]\n",
    "\n",
    "    # Infer numeric and categorical columns\n",
    "    numeric_cols, categorical_cols = _infer_datatypes(X)\n",
    "\n",
    "    # Define transformers for numerical and categorical features\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=num_imputer_strategy)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('to_string', FunctionTransformer(func=lambda x: x.astype(str), check_inverse=False)),\n",
    "        ('group_rare', FunctionTransformer(func=lambda x: x.apply(group_rare_categories, threshold=rare_category_threshold), check_inverse=False)),\n",
    "        ('imputer', SimpleImputer(strategy=cat_imputer_strategy)),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Define a column transformer that applies the appropriate transformer to each feature\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numerical_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "    # Apply the preprocessing steps to the dataset\n",
    "    X_cleaned = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Get the fitted OneHotEncoder instance\n",
    "    group_rare_encoder = categorical_transformer.named_steps['group_rare'].fit(X[categorical_cols].astype(str))\n",
    "    one_hot_encoder = categorical_transformer.named_steps['encoder'].fit(group_rare_encoder.transform(X[categorical_cols].astype(str)))\n",
    "\n",
    "    # Get the transformed categorical columns after the group_rare step\n",
    "    group_rare_transformed_columns = group_rare_encoder.transform(X[categorical_cols]).columns\n",
    "\n",
    "    # Get the feature names after the one_hot_encoder step\n",
    "    encoded_feature_names = one_hot_encoder.get_feature_names_out(group_rare_transformed_columns).tolist()\n",
    "\n",
    "    # Convert the result back to a DataFrame with appropriate column names\n",
    "    X_cleaned = pd.DataFrame(X_cleaned.toarray(), columns=numeric_cols + encoded_feature_names)\n",
    "\n",
    "    return X_cleaned, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42d3d8-b6f9-499b-9628-752ced183700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_feature = \"JV_default_PCE\"\n",
    "X_cleaned, y = preprocess_data(df=df, target_feature=target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87d1aa-ccb7-4001-b7b2-0ce1f721fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42b26c-5e68-4bf2-83b9-1d1a3e6fe724",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b56140-21da-491e-93ab-50d44a040608",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1709b-e4a6-465a-80ab-12019623fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def cross_val_regression(X, y, model, scoring, n_splits=10):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation for the given model and return the scores.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X: pd.DataFrame or np.array\n",
    "        The feature matrix.\n",
    "    y: pd.Series or np.array\n",
    "        The target variable.\n",
    "    model: scikit-learn estimator\n",
    "        The regression model to be used for cross-validation.\n",
    "    scoring: str\n",
    "        The scoring method to use. For example, 'neg_mean_squared_error' for regression tasks.\n",
    "    n_splits: int, optional (default=10)\n",
    "        The number of splits for the k-fold cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    scores: np.array\n",
    "        The cross-validation scores for the given model.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=kf, scoring=scoring)\n",
    "    return scores\n",
    "\n",
    "def naive_model_regression(y, strategy='mean'):\n",
    "    \"\"\"\n",
    "    Create a naive model for regression.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y: pd.Series or np.array\n",
    "        The target variable.\n",
    "    strategy: str, optional (default='mean')\n",
    "        The strategy to use for the naive model. Possible values are 'mean', 'median', or 'constant'.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    naive_model: DummyRegressor\n",
    "        A scikit-learn DummyRegressor instance with the specified strategy.\n",
    "    \"\"\"\n",
    "    naive_model = DummyRegressor(strategy=strategy)\n",
    "    return naive_model\n",
    "\n",
    "def compare_models(X, y, baseline_model, naive_model, scoring, greater_is_better=True):\n",
    "    \"\"\"\n",
    "    Compare the performance of baseline and naive models.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X: pd.DataFrame or np.array\n",
    "        The feature matrix.\n",
    "    y: pd.Series or np.array\n",
    "        The target variable.\n",
    "    baseline_model: scikit-learn estimator\n",
    "        The baseline regression model.\n",
    "    naive_model: scikit-learn estimator\n",
    "        The naive regression model.\n",
    "    scoring: str\n",
    "        The scoring method to use. For example, 'neg_mean_squared_error' for regression tasks.\n",
    "    greater_is_better: bool, optional\n",
    "        Whether a higher score is better or not. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None. Prints the mean and standard deviation of the cross-validation scores for both models,\n",
    "    and indicates which model performs better.\n",
    "    \"\"\"\n",
    "    baseline_scores = cross_val_regression(X, y, baseline_model, scoring)\n",
    "    naive_scores = cross_val_regression(X, y, naive_model, scoring)\n",
    "    \n",
    "    if greater_is_better:\n",
    "        print(f\"Baseline {scoring} score: {baseline_scores.mean():.4f} ± {baseline_scores.std():.4f}\")\n",
    "        print(f\"Naive {scoring} score: {naive_scores.mean():.4f} ± {naive_scores.std():.4f}\")\n",
    "        \n",
    "        if baseline_scores.mean() < naive_scores.mean():\n",
    "            print(\"Naive model performs better than the baseline model.\")\n",
    "        else:\n",
    "            print(\"Baseline model performs better than the naive model.\")\n",
    "    else:\n",
    "        baseline_scores = -baseline_scores\n",
    "        naive_scores = -naive_scores\n",
    "        \n",
    "        print(f\"Baseline model performance: {baseline_scores.mean():.4f} ± {baseline_scores.std():.4f}\")\n",
    "        print(f\"Naive model performance: {naive_scores.mean():.4f} ± {naive_scores.std():.4f}\")\n",
    "        \n",
    "        if baseline_scores.mean() > naive_scores.mean():\n",
    "            print(\"Naive model performs better than the baseline model.\")\n",
    "        else:\n",
    "            print(\"Baseline model performs better than the naive model.\")\n",
    "    \n",
    "    return baseline_scores, naive_scores\n",
    "\n",
    "\n",
    "        \n",
    "def handle_missing_target(y, strategy='mean', constant_value=None):\n",
    "    \"\"\"\n",
    "    Handle missing values in the target variable (y) using the specified strategy.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y: pd.Series or np.array\n",
    "        The target variable containing missing values.\n",
    "    strategy: str, optional (default='mean')\n",
    "        The strategy to handle missing values. Possible values are 'drop', 'mean', 'median', or 'constant'.\n",
    "    constant_value: any, optional (default=None)\n",
    "        The constant value to use when the strategy is set to 'constant'. If the strategy is 'constant' and\n",
    "        constant_value is not provided, a ValueError will be raised.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple: (imputed_y, mask)\n",
    "        imputed_y: pd.Series or np.array\n",
    "            The target variable with missing values handled according to the chosen strategy.\n",
    "        mask: np.array\n",
    "            A boolean array that can be used to filter rows in both the target variable and the feature matrix.\n",
    "            This is particularly useful when the chosen strategy is 'drop', as it allows for the removal of\n",
    "            corresponding rows in both the target variable and feature matrix.\n",
    "    \"\"\"\n",
    "    mask = np.ones(y.shape, dtype=bool)\n",
    "    \n",
    "    if strategy == 'drop':\n",
    "        mask = ~np.isnan(y)\n",
    "        return y[mask], mask\n",
    "    elif strategy == 'mean':\n",
    "        imputed_y = y.copy()\n",
    "        imputed_y[np.isnan(y)] = y.mean()\n",
    "        return imputed_y, mask\n",
    "    elif strategy == 'median':\n",
    "        imputed_y = y.copy()\n",
    "        imputed_y[np.isnan(y)] = y.median()\n",
    "        return imputed_y, mask\n",
    "    elif strategy == 'constant' and constant_value is not None:\n",
    "        imputed_y = y.copy()\n",
    "        imputed_y[np.isnan(y)] = constant_value\n",
    "        return imputed_y, mask\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy or constant_value not provided.\")\n",
    "\n",
    "\n",
    "# Choose how to handle missing values in y\n",
    "y_cleaned, mask = handle_missing_target(y, strategy='drop')\n",
    "\n",
    "# Update X_cleaned to remove corresponding rows\n",
    "X_cleaned = X_cleaned[mask]\n",
    "\n",
    "# Linear regression as a baseline model\n",
    "baseline_model = LinearRegression()\n",
    "\n",
    "# Naive model for regression\n",
    "naive_model = naive_model_regression(y_cleaned)\n",
    "\n",
    "# Scoring metric\n",
    "greater_is_better=False\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=greater_is_better)\n",
    "\n",
    "# Compare the models using the cleaned target variable and updated X_cleaned\n",
    "baseline, naive = compare_models(X_cleaned, y_cleaned, baseline_model, naive_model, mse_scorer, greater_is_better=greater_is_better)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
